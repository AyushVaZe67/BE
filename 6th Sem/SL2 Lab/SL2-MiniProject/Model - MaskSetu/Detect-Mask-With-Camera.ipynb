{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859be0b8-0082-4eb1-9a2f-78533889f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7a415a-dfec-4b9d-a246-da1fad37c9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "m1 = load_model(r\"E:\\AYUSH VAZE\\DEGREE\\Study\\6th sem\\SL2 Lab\\SL2-MiniProject\\Model - MaskSetu\\model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc0cfaa-ef53-4dd7-ab67-9379fe0a066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def detect_mask_from_frame(frame, model):\n",
    "    \"\"\"\n",
    "    Detects mask from an image frame instead of file path.\n",
    "    \"\"\"\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    img_resized = cv2.resize(img, (224, 224))  # Resize to VGG16 input size\n",
    "    img_resized = np.array(img_resized, dtype=np.float32)  # Convert to NumPy array\n",
    "\n",
    "    # Ensure 3 channels (handle grayscale images)\n",
    "    if len(img_resized.shape) == 2:\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    img_resized = preprocess_input(img_resized)  # Apply VGG16 preprocessing\n",
    "    img_resized = img_resized.reshape(1, 224, 224, 3)  # Reshape for model input\n",
    "\n",
    "    y_pred = model.predict(img_resized)\n",
    "    y_pred1 = model.predict(img_resized)\n",
    "    y_pred = (y_pred > 0.5).astype(int)  # Convert probability to 0 or 1\n",
    "    y_pred = int(y_pred[0][0])  # Convert NumPy array to integer\n",
    "\n",
    "    class_mapping = {0: \"With Mask\", 1: \"Without Mask\"}\n",
    "    return class_mapping[y_pred],y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165dbeb-a1f4-4948-a413-d6131abe413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Load your trained mask detection model\n",
    "model = load_model(\"model1.h5\")  # Replace with your actual model filename\n",
    "\n",
    "# Load OpenCV's pre-trained Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_mask_from_frame(frame, model):\n",
    "    \"\"\"\n",
    "    Detects mask from an image frame instead of file path.\n",
    "    \"\"\"\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_resized = np.array(img_resized, dtype=np.float32)\n",
    "\n",
    "    if len(img_resized.shape) == 2:\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    img_resized = preprocess_input(img_resized)\n",
    "    img_resized = img_resized.reshape(1, 224, 224, 3)\n",
    "\n",
    "    y_pred = model.predict(img_resized)\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    y_pred = int(y_pred[0][0])\n",
    "\n",
    "    class_mapping = {0: \"With Mask\", 1: \"Without Mask\"}\n",
    "    return class_mapping[y_pred]\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        label = detect_mask_from_frame(face, model)\n",
    "\n",
    "        color = (0, 255, 0) if label == \"With Mask\" else (0, 0, 255)\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Mask Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad0ff3c-3abc-4160-8256-9a553cc42b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Without Mask', array([[0.9220903]], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image from the file path\n",
    "frame = cv2.imread(r\"E:\\AYUSH VAZE\\DEGREE\\Study\\6th sem\\SL2 Lab\\SL2-MiniProject\\Personal Data\\188.png\")\n",
    "\n",
    "# Now pass the loaded image (frame) to your function\n",
    "detect_mask_from_frame(frame, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9da03e-42ae-4172-8580-aa8cc81491aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "m1.save('model1.h5')  # Save Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5598abde-9b1c-45f1-b8e2-80eb12ddc2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb8996ac-1512-4257-ba75-7a8f105b3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpq5qbpzt_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpq5qbpzt_\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\HP\\AppData\\Local\\Temp\\tmpq5qbpzt_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2513922026064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922028560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922028368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922029328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922029136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922030096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922029904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922030864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922030672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922031248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922031440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922029520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922031056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922901776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922901584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922902544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922901968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922903504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922903120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922904080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922903888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922904848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922904272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922905808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922906000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922907728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922910032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922911760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922911952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922913680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922913872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2513922915600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model converted and saved as mask_detector_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your .h5 model\n",
    "model = tf.keras.models.load_model('model1.h5')\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('mask_detector_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted and saved as mask_detector_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de950e5e-494c-497b-a646-e596557cf728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)  # Open webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret or frame is None:\n",
    "        print(\"Failed to capture frame. Exiting...\")\n",
    "        break\n",
    "\n",
    "    prediction = detect_mask_from_frame(frame, m1)  # Use modified function\n",
    "\n",
    "    if prediction == \"With Mask\":\n",
    "        cv2.putText(frame, 'With Mask', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, 'Without Mask', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Mask Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4c968-5ed1-4000-8d9b-482efe85dd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
